{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1ba6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q sentence-transformers scikit-learn numpy pandas\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# ---- ENV/paths (Gradient notebooks) ----\n",
    "OUT_DIR = Path(\"/storage/ai\")  # persistent volume in Paperspace\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"BAAI/bge-base-en-v1.5\"\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = \"cuda\" if USE_CUDA else \"cpu\"\n",
    "\n",
    "# Bigger batches on GPU; smaller on CPU\n",
    "BATCH_SIZE = 512 if USE_CUDA else 64\n",
    "\n",
    "print(\"Device:\", DEVICE, \"| GPU name:\", torch.cuda.get_device_name(0) if USE_CUDA else \"CPU\")\n",
    "\n",
    "# ---- Load data ----\n",
    "df = pd.read_json(OUT_DIR / \"articles_df.json\", orient=\"records\")\n",
    "df[\"Title\"] = df[\"Title\"].fillna(\"\").astype(str)\n",
    "df[\"Summary\"] = df[\"Summary\"].fillna(\"\").astype(str)\n",
    "texts = (df[\"Title\"] + \" \" + df[\"Summary\"]).tolist()\n",
    "ids = df[\"ID\"].astype(str).tolist()\n",
    "\n",
    "# ---- Load model on GPU ----\n",
    "model = SentenceTransformer(MODEL_NAME, device=DEVICE)\n",
    "\n",
    "# Optional: slight speed-up on GPU by enabling fp16 autocast during encode\n",
    "# SentenceTransformers will use the device; autocast here makes matmuls use fp16\n",
    "def encode_in_batches(texts, batch_size=256):\n",
    "    embs_list = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # autocast only if CUDA available\n",
    "        ctx = torch.cuda.amp.autocast(dtype=torch.float16) if USE_CUDA else torch.no_grad()\n",
    "        with ctx:\n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch = texts[i:i+batch_size]\n",
    "                emb = model.encode(\n",
    "                    batch,\n",
    "                    batch_size=len(batch),          # we already batch manually\n",
    "                    convert_to_numpy=True,\n",
    "                    normalize_embeddings=False,\n",
    "                    show_progress_bar=False,\n",
    "                )\n",
    "                embs_list.append(emb)\n",
    "    return np.vstack(embs_list)\n",
    "\n",
    "# ---- Encode ----\n",
    "embs = encode_in_batches(texts, BATCH_SIZE)\n",
    "# L2-normalize for cosine\n",
    "embs = normalize(embs, norm=\"l2\", copy=False).astype(np.float32)\n",
    "\n",
    "# ---- Save artifacts (dense) ----\n",
    "np.save(OUT_DIR / \"bge_embeddings.npy\", embs)\n",
    "with open(OUT_DIR / \"ids_by_row.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(ids, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(OUT_DIR / \"embedding_manifest.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"dim\": int(embs.shape[1]),\n",
    "        \"normalized\": True,\n",
    "        \"ids_by_row\": str(OUT_DIR / \"ids_by_row.json\"),\n",
    "        \"embeddings_npy\": str(OUT_DIR / \"bge_embeddings.npy\"),\n",
    "        \"note\": \"Rows in embeddings.npy align with ids_by_row.json.\"\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved:\", OUT_DIR / \"bge_embeddings.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad38cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Reads embeddings from /storage/ai/bge_embeddings.npy (L2-normalized float32)\n",
    "# - Recursively splits clusters > MAX_CLUSTER_SIZE using MiniBatchKMeans\n",
    "# - Saves per-cluster vectors + ids + centroids for query routing\n",
    "\n",
    "import json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# ------------------------------\n",
    "# CONFIG\n",
    "# ------------------------------\n",
    "OUT_DIR = Path(\"/storage/ai\")              # <-- Paperspace persistent volume\n",
    "INITIAL_K = 24\n",
    "MAX_CLUSTER_SIZE = 1000\n",
    "RANDOM_STATE = 42\n",
    "BATCH_SIZE = 4096\n",
    "MAX_ITER = 200\n",
    "N_INIT = 20\n",
    "SHARD_DTYPE = \"float32\"                    # set to \"float16\" to halve size\n",
    "\n",
    "# ------------------------------\n",
    "# LOAD\n",
    "# ------------------------------\n",
    "embs_path = OUT_DIR / \"bge_embeddings.npy\"\n",
    "ids_path  = OUT_DIR / \"ids_by_row.json\"\n",
    "manifest_path = OUT_DIR / \"embedding_manifest.json\"\n",
    "\n",
    "assert embs_path.exists(), f\"Embeddings not found: {embs_path}\"\n",
    "assert ids_path.exists(),  f\"IDs not found: {ids_path}\"\n",
    "\n",
    "embs = np.load(embs_path)                  # (n, d), L2-normalized float32\n",
    "with open(ids_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    ids_by_row = json.load(f)\n",
    "\n",
    "# Optional: read model name from embedding manifest (for provenance)\n",
    "MODEL_NAME = None\n",
    "if manifest_path.exists():\n",
    "    try:\n",
    "        with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            MODEL_NAME = json.load(f).get(\"model\")\n",
    "    except Exception:\n",
    "        MODEL_NAME = None\n",
    "\n",
    "n, d = embs.shape\n",
    "print(f\"Embeddings: {embs.shape} | dtype={embs.dtype} | shards dtype={SHARD_DTYPE}\")\n",
    "\n",
    "# ------------------------------\n",
    "# HELPERS\n",
    "# ------------------------------\n",
    "def run_kmeans_dense(X_sub, k):\n",
    "    km = MiniBatchKMeans(\n",
    "        n_clusters=k, random_state=RANDOM_STATE,\n",
    "        batch_size=BATCH_SIZE, max_iter=MAX_ITER,\n",
    "        n_init=N_INIT, init=\"k-means++\", verbose=0\n",
    "    )\n",
    "    labels = km.fit_predict(X_sub)\n",
    "    centers = km.cluster_centers_\n",
    "    # normalize centers for cosine-friendly dot\n",
    "    centers /= (np.linalg.norm(centers, axis=1, keepdims=True) + 1e-12)\n",
    "    return labels, centers\n",
    "\n",
    "def split_count(size, cap): \n",
    "    return max(2, math.ceil(size / cap))\n",
    "\n",
    "# ------------------------------\n",
    "# FIRST PASS + RECURSIVE SPLIT\n",
    "# ------------------------------\n",
    "labels0, centers0 = run_kmeans_dense(embs, INITIAL_K)\n",
    "clusters = [np.where(labels0 == c)[0] for c in range(INITIAL_K)]\n",
    "\n",
    "final_clusters = []\n",
    "final_centroids = []\n",
    "queue = list(range(len(clusters)))\n",
    "\n",
    "while queue:\n",
    "    i = queue.pop()\n",
    "    idx = clusters[i]\n",
    "    if idx.size == 0:\n",
    "        continue\n",
    "    if idx.size <= MAX_CLUSTER_SIZE:\n",
    "        # centroid = normalized mean\n",
    "        centroid = embs[idx].mean(axis=0)\n",
    "        norm = np.linalg.norm(centroid)\n",
    "        if norm > 0: centroid /= norm\n",
    "        final_clusters.append(idx)\n",
    "        final_centroids.append(centroid.astype(np.float32))\n",
    "    else:\n",
    "        X_sub = embs[idx]\n",
    "        k_sub = split_count(idx.size, MAX_CLUSTER_SIZE)\n",
    "        sub_labels, _ = run_kmeans_dense(X_sub, k_sub)\n",
    "        for sc in range(k_sub):\n",
    "            sub_idx = idx[np.where(sub_labels == sc)[0]]\n",
    "            clusters.append(sub_idx)\n",
    "            queue.append(len(clusters) - 1)\n",
    "\n",
    "# ------------------------------\n",
    "# SAVE SHARDS + INDEXES\n",
    "# ------------------------------\n",
    "cluster_index = {}\n",
    "for k, idx in enumerate(final_clusters, start=1):\n",
    "    vecs_path = OUT_DIR / f\"emb_cluster_{k}.npy\"\n",
    "    ids_out   = OUT_DIR / f\"ids_cluster_{k}.json\"\n",
    "\n",
    "    # Optionally store shard vectors as float16 to save space\n",
    "    shard = embs[idx].astype(np.float16 if SHARD_DTYPE == \"float16\" else np.float32)\n",
    "    np.save(vecs_path, shard)\n",
    "    with open(ids_out, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump([str(ids_by_row[i]) for i in idx], f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    cluster_index[str(k)] = {\n",
    "        \"size\": int(idx.size),\n",
    "        \"vectors_path\": str(vecs_path),\n",
    "        \"ids_path\": str(ids_out)\n",
    "    }\n",
    "\n",
    "centroids = np.vstack(final_centroids) if final_centroids else np.zeros((0, d), dtype=np.float32)\n",
    "np.save(OUT_DIR / \"centroids.npy\", centroids.astype(np.float32))   # keep centroids in fp32\n",
    "\n",
    "# Row → cluster map\n",
    "row_to_cluster = np.empty(n, dtype=np.int32)\n",
    "for cluster_id, idx in enumerate(final_clusters, start=1):\n",
    "    row_to_cluster[idx] = cluster_id\n",
    "id_to_cluster = {str(ids_by_row[i]): int(row_to_cluster[i]) for i in range(n)}\n",
    "with open(OUT_DIR / \"id_to_cluster.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(id_to_cluster, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Sizes CSV\n",
    "pd.DataFrame(\n",
    "    {\"cluster_id\": list(map(int, cluster_index.keys())), \n",
    "     \"size\": [v[\"size\"] for v in cluster_index.values()]}\n",
    ").to_csv(OUT_DIR / \"cluster_sizes_embeddings.csv\", index=False)\n",
    "\n",
    "# Manifest\n",
    "with open(OUT_DIR / \"emb_kmeans_manifest.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"model\": MODEL_NAME or \"unknown\",\n",
    "        \"centroids_npy\": str(OUT_DIR / \"centroids.npy\"),\n",
    "        \"cluster_index\": cluster_index,\n",
    "        \"id_to_cluster\": str(OUT_DIR / \"id_to_cluster.json\"),\n",
    "        \"cluster_sizes_csv\": str(OUT_DIR / \"cluster_sizes_embeddings.csv\"),\n",
    "        \"notes\": {\n",
    "            \"l2_norm\": \"All vectors and centroids L2-normalized; use dot product for cosine.\",\n",
    "            \"routing\": \"Pick top 1–3 nearest centroids for query; then search in those shards.\",\n",
    "            \"shard_dtype\": SHARD_DTYPE\n",
    "        }\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Final clusters: {len(final_clusters)} | \"\n",
    "      f\"min={min([arr.size for arr in final_clusters])} \"\n",
    "      f\"max={max([arr.size for arr in final_clusters])} | \"\n",
    "      f\"saved under {OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33c4b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query → nearest centroids → shard search (with full-record printing)\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "OUT_DIR = Path(\"/storage/ai\")\n",
    "MODEL_NAME = \"BAAI/bge-base-en-v1.5\"\n",
    "\n",
    "# --- Build ID -> full record index (rebuilt every run) ---\n",
    "def build_record_index(out_dir: Path):\n",
    "    df_path = out_dir / \"articles_df.json\"\n",
    "    df = pd.read_json(df_path, orient=\"records\")\n",
    "    # be robust to either 'ID' or 'ArticleId'\n",
    "    id_col = \"ID\" if \"ID\" in df.columns else (\"ArticleId\" if \"ArticleId\" in df.columns else None)\n",
    "    if id_col is None:\n",
    "        raise KeyError(\"Neither 'ID' nor 'ArticleId' column found in articles_df.json\")\n",
    "    # ensure strings for keys\n",
    "    df[id_col] = df[id_col].astype(str)\n",
    "    # optional: drop NaNs for prettier output\n",
    "    return {row[id_col]: {k: v for k, v in row.items() if pd.notna(v)} for row in df.to_dict(orient=\"records\")}\n",
    "\n",
    "record_by_id = build_record_index(OUT_DIR)\n",
    "\n",
    "# --- Load model (CPU is fine; use 'cuda' if available) ---\n",
    "model = SentenceTransformer(MODEL_NAME, device=\"cpu\")\n",
    "\n",
    "# --- Load centroids (0-based rows = final clusters) ---\n",
    "centroids = np.load(OUT_DIR / \"centroids.npy\", allow_pickle=False).astype(np.float32)\n",
    "\n",
    "def embed_query(text: str):\n",
    "    \"\"\"Embed a query string with BGE, normalize for cosine dot products.\"\"\"\n",
    "    q = model.encode([text], convert_to_numpy=True, normalize_embeddings=False)\n",
    "    q = normalize(q, norm=\"l2\", copy=False).astype(np.float32)\n",
    "    return q  # shape (1, d)\n",
    "\n",
    "def top_centroids(q_vec, topn=3):\n",
    "    \"\"\"Return indices (0-based) of nearest centroids and their sims.\"\"\"\n",
    "    sims = (q_vec @ centroids.T).ravel()   # cosine (dot product; both L2-normalized)\n",
    "    order = np.argsort(-sims)[:topn]\n",
    "    return order, sims[order]\n",
    "\n",
    "def _load_shard_vectors(vecs_path: Path) -> np.ndarray:\n",
    "    \"\"\"Load shard vectors saved as float32 or float16; return float32.\"\"\"\n",
    "    arr = np.load(vecs_path)\n",
    "    if arr.dtype != np.float32:\n",
    "        arr = arr.astype(np.float32)\n",
    "    return arr  # (N_shard, d) L2-normalized\n",
    "\n",
    "def search_shards(q_vec, cluster_ids_1based, topk=10):\n",
    "    \"\"\"Search within given 1-based cluster ids; return sorted topk matches.\"\"\"\n",
    "    results = []\n",
    "    for cluster_id in cluster_ids_1based:\n",
    "        ids_path = OUT_DIR / f\"ids_cluster_{cluster_id}.json\"\n",
    "        vecs_path = OUT_DIR / f\"emb_cluster_{cluster_id}.npy\"\n",
    "\n",
    "        # load shard ids and vectors\n",
    "        with open(ids_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            shard_ids = json.load(f)\n",
    "        Xc = _load_shard_vectors(vecs_path)  # float32\n",
    "\n",
    "        # cosine = dot since everything is L2-normalized\n",
    "        scores = (q_vec @ Xc.T).ravel()\n",
    "        local_top = np.argsort(-scores)[:topk]\n",
    "        for i in local_top:\n",
    "            if scores[i] > 0:\n",
    "                results.append({\n",
    "                    \"id\": str(shard_ids[i]),\n",
    "                    \"score\": float(scores[i]),\n",
    "                    \"cluster_id\": int(cluster_id)\n",
    "                })\n",
    "\n",
    "    results.sort(key=lambda r: -r[\"score\"])\n",
    "    return results[:topk]\n",
    "\n",
    "# --- Example query ---\n",
    "query = (\n",
    "    \"Many people believe there was a conspiracy to kill JFK involving high-ranking officials. \"\n",
    "    \"Declassified 2017 documents mentioned internal agency awareness of plans prior to Nov 22, 1963.\"\n",
    ")\n",
    "q_vec = embed_query(query)\n",
    "\n",
    "# Route to nearest 1–3 centroids (centroids are 0-based; files are 1-based)\n",
    "centroid_order, centroid_scores = top_centroids(q_vec, topn=3)\n",
    "candidate_clusters = [int(i) + 1 for i in centroid_order]\n",
    "\n",
    "# Search in those clusters\n",
    "hits = search_shards(q_vec, candidate_clusters, topk=10)\n",
    "\n",
    "# Print line + full JSON for each hit\n",
    "for h in hits[:10]:\n",
    "    _id = h[\"id\"]\n",
    "    cluster_id = h[\"cluster_id\"]\n",
    "    score = h[\"score\"]\n",
    "    print(f\"[cluster {cluster_id}] {_id}  score={score:.4f}\")\n",
    "\n",
    "    rec = record_by_id.get(_id)\n",
    "    if rec is not None:\n",
    "        print(json.dumps(rec, ensure_ascii=False, indent=2))\n",
    "    else:\n",
    "        print(f\"(No full record found for ID={_id})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
