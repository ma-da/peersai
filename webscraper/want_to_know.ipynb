{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f649080-91fe-4b8b-bc7a-b24610348205",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello World!\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52752c83-eafe-4932-aec4-e1e6aa6c9460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cec86f56-d967-42d4-87d2-7557f63e6a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.25.3-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.25.3-cp39-abi3-win_amd64.whl (16.5 MB)\n",
      "   ---------------------------------------- 0.0/16.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/16.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.0/16.5 MB 3.3 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.6/16.5 MB 3.0 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 2.1/16.5 MB 2.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 2.6/16.5 MB 2.7 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 2.9/16.5 MB 2.7 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 3.7/16.5 MB 2.6 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 4.2/16.5 MB 2.6 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 4.5/16.5 MB 2.6 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 5.0/16.5 MB 2.5 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 5.2/16.5 MB 2.5 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 5.8/16.5 MB 2.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 6.6/16.5 MB 2.5 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 7.1/16.5 MB 2.5 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 7.6/16.5 MB 2.6 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 8.4/16.5 MB 2.6 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 8.9/16.5 MB 2.6 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 9.7/16.5 MB 2.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 10.5/16.5 MB 2.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 11.0/16.5 MB 2.7 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 11.3/16.5 MB 2.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 11.3/16.5 MB 2.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 11.5/16.5 MB 2.5 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 12.1/16.5 MB 2.5 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 12.1/16.5 MB 2.5 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 12.1/16.5 MB 2.5 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 12.3/16.5 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 12.8/16.5 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 12.8/16.5 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 13.1/16.5 MB 2.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 13.1/16.5 MB 2.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 13.4/16.5 MB 2.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 13.6/16.5 MB 2.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 14.2/16.5 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 14.2/16.5 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 14.4/16.5 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 14.9/16.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 15.5/16.5 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 16.0/16.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.5/16.5 MB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.25.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f548ad2-c110-4374-88f3-8d6a7618ee15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7f4364b-84be-4636-b7cd-577b0e87cf61",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fitz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#PDF retriever\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfitz\u001b[39;00m  \u001b[38;5;66;03m# PyMuPDF\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_pdf\u001b[39m(url, output_path):\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Download a PDF file from a URL and save it locally.\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fitz'"
     ]
    }
   ],
   "source": [
    "#PDF retriever\n",
    "\n",
    "import requests\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def download_pdf(url, output_path):\n",
    "    \"\"\"Download a PDF file from a URL and save it locally.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(output_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"PDF downloaded and saved to {output_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download PDF. Status code: {response.status_code}\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file using PyMuPDF.\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        # Open the PDF file\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            # Iterate through each page\n",
    "            for page in doc:\n",
    "                # Extract text from the page\n",
    "                text += page.get_text()\n",
    "        print(\"Text extracted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text: {e}\")\n",
    "    return text\n",
    "\n",
    "def save_text_to_file(text, output_path):\n",
    "    \"\"\"Save extracted text to a file.\"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "    print(f\"Text saved to {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # URL of the PDF file\n",
    "    pdf_url = \"https://example.com/sample.pdf\"  # Replace with the actual URL\n",
    "    \n",
    "    # Local paths\n",
    "    pdf_output_path = \"sample.pdf\"  # Path to save the downloaded PDF\n",
    "    text_output_path = \"output.txt\"  # Path to save the extracted text\n",
    "\n",
    "    # Step 1: Download the PDF\n",
    "    download_pdf(pdf_url, pdf_output_path)\n",
    "\n",
    "    # Step 2: Extract text from the PDF\n",
    "    extracted_text = extract_text_from_pdf(pdf_output_path)\n",
    "\n",
    "    # Step 3: Save the extracted text to a file\n",
    "    save_text_to_file(extracted_text, text_output_path)\n",
    "\n",
    "    # Print the extracted text (optional)\n",
    "    print(\"Extracted Text:\\n\", extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f172c70-b193-4edf-82da-37d336128941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142e4ad4-f182-4df8-bcbc-bb3a83455548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TITLE: PEERS AI Project Website Scraper (WantToKnowScraper)\n",
    "# VERSION: 1.0\n",
    "# AUTHOR: Marc Baber\n",
    "# DATE: 06 MAR 2025\n",
    "# \n",
    "# TO DO LIST:\n",
    "# 1. Make FIFO stack, for breadth first, not depth\n",
    "# 2. No hash suffixes\n",
    "# 3. Don't take HREFs from archived docs\n",
    "# 4. Need a way to detect and avoid recursion. PRUNE???\n",
    "# 5. If file already local, load it insteaed???\n",
    "# 6. Prune out MK docs\n",
    "#\n",
    "# \n",
    "# 1. Fix MomentOfLove navbar recursions on g/victim_or_creator_vs and inspiration/inspiring-videos \n",
    "# 2. Set recursion limit higher (apparently 1000 wasn't high enough). But is there a way to avoid getting so deep?\n",
    "#    I might have to go breadth first (instead of depth first) using a FIFO set/stack\n",
    "#    a. seed the FIFO with just the first (base) URL\n",
    "#    b. WHILE there's anything in the FIFO queue that has not already been visited:\n",
    "#       crawl the first URL:\n",
    "#       retrieve file (html or pdf) -- getting archive if necessary\n",
    "#       make .txt file\n",
    "#       If HTML, add all hrefs to FIFO stack, but not if archive (push right)\n",
    "#       Add this url to visited.\n",
    "#       WEND\n",
    "# 3. Don't visit a # hash href if the base URL has already been visited\n",
    "# 4. Why did processing stop after 50 hrs on a PPT file: http://www.cs.cmu.edu/~pausch/Randy/Randy/pauschlastlecture.ppt ? \n",
    "#    (update: did not crash, just stalled)\n",
    "# 5. \n",
    "\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Wayback Machine API endpoint\n",
    "WAYBACK_API = \"http://archive.org/wayback/available\"\n",
    "headers = { \"User-Agent\": \"AiBot/1.0\" }\n",
    "\n",
    "# Don't prune page tree while still on \"home site\" which should include whole family of PEERS  \n",
    "# Websites and online courses currently managed by PEERS:\n",
    "# \n",
    "# www.momentoflove.org - Every person in the world has a heart\n",
    "# www.weboflove.org - Strengthening the web of love that interconnects us all\n",
    "# www.WantToKnow.info - Revealing major cover-ups and working together for a brighter future\n",
    "# www.newsarticles.media - Collection of under-reported major media news articles\n",
    "# www.divinemystery.net - Mystical musings of a spiritual explorer\n",
    "# www.inspiringcommunity.org - Building a global community for all\n",
    "# www.wisdomcourses.net - Free online courses inspire you to greatness\n",
    "# www.inspirationcourse.net - The Inspiration Course: Opening to more love and deeper connection\n",
    "# www.hidden-knowledge.net - Hidden Knowledge Course: Illuminating shadow aspects of our world\n",
    "# www.insightcourse.net - The Insight Course: The best of the Internet all in one free course\n",
    "# www.transformationteam.net - Transformation Team: Building bridges to expanded consciousness\n",
    "# www.gatheringspot.net - Dynamic community networking portal for course graduates\n",
    "\n",
    "pattern_peers_family = re.compile(r\"\"\"\n",
    "    ^https?://        # Start with http or https\n",
    "    www\\.             # Literal 'www.'\n",
    "    (                 # Start of group for domain names\n",
    "        wanttoknow\\.info         |\n",
    "        # momentoflove\\.org        | \n",
    "        # momentoflove domain taken out 03/06/25 due to recursive href's in the navbar in two places:\n",
    "        # 1. g/victim_or_creator_vs lead to \"g/g/g/g/g/g/g/g\" recursions\n",
    "        # 2. inspiration/inspiring-videos lead to \"inspiration/inspiration/inspiration...\" recursions\n",
    "        weboflove\\.org           |\n",
    "        newsarticles\\.media      |\n",
    "        divinemystery\\.net       |\n",
    "        inspiringcommunity\\.org  |\n",
    "        wisdomcourses\\.net       |\n",
    "        inspirationcourse\\.net   |\n",
    "        hidden-knowledge\\.net    |\n",
    "        insightcourse\\.net       |\n",
    "        transformationteam\\.net  |\n",
    "        gatheringspot\\.net\n",
    "    )                # End of group\n",
    "    \"\"\", re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "pattern_archive_url = re.compile(r\"\"\"\n",
    "    ^https?://        # Start with http or https\n",
    "    web\\.archive\\.org        \n",
    "    \"\"\", re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "pattern_hash_url = r'#([\\w-]+)$'\n",
    "\n",
    "def download_pdf(url, output_path):\n",
    "    \"\"\"Download a PDF file from a URL and save it locally.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(output_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"PDF downloaded and saved to {output_path}\", flush=True)\n",
    "    else:\n",
    "        print(f\"Failed to download PDF. Status code: {response.status_code}\", flush=True)\n",
    "        \n",
    "def download_url(url, path):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    with open(path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "def get_wayback_url(url):\n",
    "    \"\"\"Fetch the closest archived version of a URL from Archive.org.\"\"\"\n",
    "    params = {\"url\": url}\n",
    "    response = requests.get(WAYBACK_API, params=params)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        if \"archived_snapshots\" in result and \"closest\" in result[\"archived_snapshots\"]:\n",
    "            return result[\"archived_snapshots\"][\"closest\"][\"url\"]\n",
    "    else:\n",
    "        print(f\"No archived snapshot found: {url}\", flush=True)\n",
    "    return None\n",
    "\n",
    "\n",
    "def html_to_text(mysoup):\n",
    "    # print(f\"Enter html_to_text\", flush=True)\n",
    "    \n",
    "    # Remove script and style elements\n",
    "    for script_or_style in mysoup(['script', 'style']):\n",
    "        script_or_style.decompose()\n",
    "    \n",
    "    # Extract text\n",
    "    text = mysoup.get_text(separator=' ')\n",
    "    \n",
    "    # Clean up whitespace\n",
    "    lines = (line.strip() for line in text.splitlines())  # Remove leading/trailing whitespace\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))  # Split multi-spaces\n",
    "    text = ' '.join(chunk for chunk in chunks if chunk)  # Join non-empty chunks\n",
    "\n",
    "    # print(f\"Exit html_to_text\", flush=True)\n",
    "    return text\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file using PyMuPDF.\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        # Open the PDF file\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            # Iterate through each page\n",
    "            for page in doc:\n",
    "                # Extract text from the page\n",
    "                text += page.get_text()\n",
    "        print(\"Text extracted successfully.\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text: {e}\", flush=True)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def crawl_site(start_url, output_dir, max_depth=2):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    visited = set()\n",
    "\n",
    "    def crawl(url, depth_actual, depth_effective):\n",
    "        \n",
    "        if pattern_peers_family.match(url):\n",
    "            depth_effective = 0  # Effective depth is how many hops from home domain(s)\n",
    "            print(f\"URL is in Home Domain(s): {url}\", flush=True)\n",
    "        else:\n",
    "            print(f\"URL is NOT in Home Domain(s) {url}\", flush=True)\n",
    "\n",
    "        if pattern_archive_url.match(url):\n",
    "            print(f\"SKIPPING Archive URL {url}\", flush=True)\n",
    "            return\n",
    "\n",
    "        if url in visited:\n",
    "            print(f\"Previously visited: {url}\", flush=True)\n",
    "            return   \n",
    "        print(f\"Adding {url} to visited set\")\n",
    "        visited.add(url)\n",
    "\n",
    "        # don't stray too far from home domain(s)\n",
    "        if depth_effective > max_depth:\n",
    "            print(f\"Effective depth exceeded {depth_effective}\", flush=True)\n",
    "            return \n",
    "\n",
    "        # Don't process image files\n",
    "        if ( bool(re.search('.jpe+g$',url)) or bool(re.search('.gif$',url)) or bool(re.search('.png$',url)) ):\n",
    "            print(f\"SKIPPING IMAGE: {url}\", flush=True)\n",
    "            return\n",
    "\n",
    "        # Don't process mailto's\n",
    "        if ( bool(re.search('^mailto:',url) ) ):\n",
    "            print(f\"SKIPPING MAILTO: {url}\", flush=True)\n",
    "            return\n",
    "        \n",
    "        # print(f\"({depth_actual}/{depth_effective}) CRAWLING: {url}\", flush=True)\n",
    "\n",
    "        # Try to fetch the page\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=15 )\n",
    "            if response.status_code == 200:\n",
    "                clean_url = url.replace('https://','')\n",
    "                clean_url = clean_url.replace('http://','')\n",
    "                clean_url = clean_url.rstrip('/')\n",
    "\n",
    "                content_type = response.headers.get('Content-Type')\n",
    "                    \n",
    "                if 'application/pdf' in content_type:\n",
    "                    \n",
    "                    print(f\"File appears to be PDF {url}\", flush=True)\n",
    "                    \n",
    "                    pdf_output_path = os.path.join(output_dir, clean_url.replace('/', '_') + '.pdf')\n",
    "                    pdf_output_txt_filename = pdf_output_path.replace('.pdf','.txt')\n",
    "                    print(f\"Save PDF-to-text: {pdf_output_txt_filename}\", flush=True)\n",
    "                        \n",
    "                    # Step 1: Download the PDF\n",
    "                    download_pdf(url, pdf_output_path)\n",
    "\n",
    "                    # Step 2: Extract text from the PDF\n",
    "                    extracted_text = extract_text_from_pdf(pdf_output_path)\n",
    "\n",
    "                    # Step 3: Save the extracted text to a file\n",
    "                    save_text_to_file(extracted_text, pdf_output_txt_filename)\n",
    "                    \n",
    "                elif 'text/html' in content_type:\n",
    "                 \n",
    "                    print(f\"File appears to be HTML {url}\", flush=True)\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    filename = os.path.join(output_dir, clean_url.replace('/', '_') + '.html')\n",
    "                    # Save the page\n",
    "                    print(f\"Save page filename: {filename}\", flush=True)\n",
    "                    download_url(url, filename)\n",
    "                    filename = filename.replace('.html','.txt')\n",
    "                    print(f\"Save text: {filename}\", flush=True)\n",
    "                    with open(filename, 'wb') as file:\n",
    "                        file.write(html_to_text(soup).encode(\"utf-8\"))\n",
    "                        \n",
    "                    # Crawl internal and external links\n",
    "                    for link in soup.find_all('a', href=True):\n",
    "                        full_url = urljoin(url, link['href'])\n",
    "                        \n",
    "                        # Remove the hash and the following alphanumeric (or dash) characters at the end of the string (if any)\n",
    "                        full_url = re.sub(pattern_hash_url, '', full_url)\n",
    "                    \n",
    "                        # if full_url not in visited and full_url <> url:\n",
    "                        if full_url not in visited:\n",
    "                            print(f\"CRAWL:({depth_actual}/{depth_effective}) Parent: {url} Child: {full_url}\", flush=True)\n",
    "                            crawl(full_url, depth_actual + 1, depth_effective + 1 )\n",
    "\n",
    "                elif 'application/xml' in content_type or 'text/xml' in content_type:    \n",
    "                    print(f\"File appears to be XML {url}\", flush=True)    \n",
    "                elif 'text/css' in content_type:\n",
    "                    print(f\"File appears to be CSS {url}\", flush=True)  \n",
    "                elif 'application/javascript' in content_type or 'text/javascript' in content_type:\n",
    "                    print(f\"File appears to be Javascript {url}\", flush=True)  \n",
    "                elif 'image/jpeg' in content_type:\n",
    "                    print(f\"File appears to be JPEG image {url}\", flush=True)  \n",
    "                elif 'image/png' in content_type:\n",
    "                    print(f\"File appears to be PNG image {url}\", flush=True)  \n",
    "                elif 'image/gif' in content_type:\n",
    "                    print(f\"File appears to be GIF image {url}\", flush=True)\n",
    "                elif 'application/vnd.ms-powerpoint' in content_type:\n",
    "                    print(f\"File appears to be PPT Powerpoint {url}\", flush=True)  \n",
    "                else: # Unknown type\n",
    "                    print(f\"SKIPPING NOT HTML/PDF/XML: {url}\", flush=True) \n",
    "                # endif content_type\n",
    "            \n",
    "            else: # status code not 200\n",
    "                # Handle broken link\n",
    "                print(f\"Broken link: {url} (Status: {response.status_code})\", flush=True)\n",
    "                archived_url = get_wayback_url(url)\n",
    "                if archived_url:\n",
    "                    print(f\"Retrieving archived version from: {archived_url}\", flush=True)\n",
    "                    clean_url = archived_url.replace('https://','')\n",
    "                    clean_url = clean_url.replace('http://','')\n",
    "                    clean_url = clean_url.rstrip('/')\n",
    "                    clean_url = clean_url.replace('?','QQ')\n",
    "                    clean_url = clean_url.replace('=','EQ')\n",
    "                    clean_url = clean_url.replace('&','AMP')\n",
    "                    download_url(archived_url, os.path.join(output_dir, \"archived_\" + clean_url.replace('/', '_'))) # handle html or pdf?\n",
    "                    # download_url(archived_url, os.path.join(output_dir, \"archived_\" + clean_url.replace('/', '_') + '.html'))\n",
    "                    # Unless it's a PDF and not an HTML file ???\n",
    "                else:\n",
    "                    print(f\"ERROR: No archived version found for: {url}\", flush=True)\n",
    "                # endif for if archived\n",
    "            # endif for status code 200\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"ERROR: The request for {url} timed out\", flush=True)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR EXCEPTION WHILE CRAWLING {url}: {e}\", flush=True)\n",
    "\n",
    "    crawl(start_url, 0, 0)\n",
    "\n",
    "# Redirect all output to log file.\n",
    "file  = open(\"C:\\\\Users\\\\rames\\\\ai\\\\CrawlTest\\\\WantToKnowScraper.log\", \"a\")\n",
    "sys.stdout = file\n",
    "\n",
    "sys.setrecursionlimit(2000) # Is this truly necessary? Why wasn't 1000 enough? \n",
    "    \n",
    "print(f\"CRAWL SITE BEGIN\", flush=True)\n",
    "\n",
    "crawl_site(\"http://www.wanttoknow.info\", \"C:\\\\Users\\\\rames\\\\ai\\\\CrawlTest\\\\\")\n",
    "# crawl_site(\"https://www.wanttoknow.info/a-why-healthy-food-so-expensive-america-blame-farm-bill-congress-always-renews-make-burgers-cheaper-than-salad\", \"C:\\\\Users\\\\rames\\\\ai\\\\CrawlTest\\\\\")\n",
    "# crawl_site(\"http://www.washingtonpost.com/wp-dyn/articles/A49449-2004Dec8.html\", \"C:\\\\Users\\\\marc\\\\ai\\\\CrawlTest\\\\\")\n",
    "# crawl_site(\"http://martintruther.substack.com\", \"D:\\\\Dropbox\\\\DeepSeek\\\\CrawlTest\\\\\")\n",
    "# crawl_site(\"https://www.newschool.edu/\", \"D:\\\\Dropbox\\\\DeepSeek\\\\CrawlTest\\\\\")\n",
    "# crawl_site(\"https://explore.whatismybrowser.com/useragents/parse/\", \"D:\\\\Dropbox\\\\DeepSeek\\\\CrawlTest\\\\\")\n",
    "\n",
    "print(f\"CRAWL SITE END\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d06579-2c10-4ec7-a27b-84672a8966a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bd4d21-a034-45db-99d5-56ce5a6d62f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
