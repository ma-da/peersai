{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b451b719-7a0f-4f69-b315-62686cf2d683",
   "metadata": {},
   "source": [
    "# Install deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "377a70ba-37d1-42fb-a9a6-c4914bcd9c12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T21:07:23.256953Z",
     "iopub.status.busy": "2025-08-28T21:07:23.256700Z",
     "iopub.status.idle": "2025-08-28T21:07:24.779572Z",
     "shell.execute_reply": "2025-08-28T21:07:24.778743Z",
     "shell.execute_reply.started": "2025-08-28T21:07:23.256932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers>=4.43 in /opt/conda/lib/python3.11/site-packages (4.55.4)\n",
      "Requirement already satisfied: accelerate>=0.30 in /opt/conda/lib/python3.11/site-packages (1.10.1)\n",
      "Requirement already satisfied: peft>=0.11.1 in /opt/conda/lib/python3.11/site-packages (0.17.1)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.11/site-packages (0.6.2)\n",
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.11/site-packages (0.34.4)\n",
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.11/site-packages (0.47.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers>=4.43) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.43) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.43) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.43) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.43) (2025.7.34)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers>=4.43) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.43) (0.21.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.43) (4.67.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.30) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.30) (2.7.0+cu126)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (1.1.8)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.30) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.30) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.30) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.30) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.30) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.30) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.30) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.30) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.30) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.30) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.30) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.30) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.30) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.30) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.30) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.30) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.30) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.30) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/conda/lib/python3.11/site-packages (from triton==3.3.0->torch>=2.0.0->accelerate>=0.30) (75.8.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers>=4.43) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers>=4.43) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers>=4.43) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers>=4.43) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=0.30) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.30) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U \"transformers>=4.43\" \"accelerate>=0.30\" \"peft>=0.11.1\" safetensors huggingface_hub bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6cba74-9f16-40f1-924e-c6483a950e6d",
   "metadata": {},
   "source": [
    "# 1) Imports & basic config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d068318e-73d3-4ba3-bce6-0eae5db64c70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T21:07:55.361166Z",
     "iopub.status.busy": "2025-08-28T21:07:55.360851Z",
     "iopub.status.idle": "2025-08-28T21:08:00.111275Z",
     "shell.execute_reply": "2025-08-28T21:08:00.110601Z",
     "shell.execute_reply.started": "2025-08-28T21:07:55.361144Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic config set\n"
     ]
    }
   ],
   "source": [
    "import os, torch, shutil\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "# ---- Edit these IDs as needed ----\n",
    "BASE_ID    = \"deepseek-ai/deepseek-r1-distill-qwen-32b\"          # base model\n",
    "ADAPTER_ID = \"peers-ai/deepseek-32b-my-lora1-with-stops\"          # your LoRA repo\n",
    "OUT_DIR    = \"./merged-deepseek-32b-with-stops\"                   # where to save merged\n",
    "PUSH_TO    = \"\"  # e.g., \"yourname/merged-deepseek-32b-with-stops\" or leave \"\" to skip pushing\n",
    "\n",
    "# dtype options: \"auto\", \"bfloat16\", \"float16\", \"float32\"\n",
    "DTYPE = \"bfloat16\"\n",
    "\n",
    "# Device map: \"auto\" tries to use GPU; use \"cpu\" if you want a CPU-only merge (needs lots of RAM)\n",
    "DEVICE_MAP = \"auto\"\n",
    "\n",
    "# Set True to load the base in 8-bit (saves VRAM, slower; requires bitsandbytes)\n",
    "USE_8BIT_BASE = False\n",
    "\n",
    "# DeepSeek/Qwen stacks often need trust_remote_code=True\n",
    "TRUST_REMOTE_CODE = True\n",
    "\n",
    "print(\"Basic config set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575de473-c612-4f28-bf76-6093a4065af1",
   "metadata": {},
   "source": [
    "# 2) Login to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a03522f4-d077-4470-9fab-9bd558a362ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T21:08:55.033519Z",
     "iopub.status.busy": "2025-08-28T21:08:55.033233Z",
     "iopub.status.idle": "2025-08-28T21:08:55.060110Z",
     "shell.execute_reply": "2025-08-28T21:08:55.059477Z",
     "shell.execute_reply.started": "2025-08-28T21:08:55.033500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login success\n"
     ]
    }
   ],
   "source": [
    "# Only needed if your base/adapter are gated/private or if you'll push to Hub.\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\", \"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\")  # set in environment or paste here\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"Login success\")\n",
    "else:\n",
    "    print(\"HF_TOKEN not set—continuing without Hub login.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b15fe8-bf71-40b5-9deb-086a37ffd05c",
   "metadata": {},
   "source": [
    "# 3) Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "257ae943-3eaa-493e-81b0-199763fffb81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T21:09:17.116117Z",
     "iopub.status.busy": "2025-08-28T21:09:17.115861Z",
     "iopub.status.idle": "2025-08-28T21:09:18.016277Z",
     "shell.execute_reply": "2025-08-28T21:09:18.015645Z",
     "shell.execute_reply.started": "2025-08-28T21:09:17.116099Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4bd7f72634141bda1ec37d6a5253034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60cf0ab7f7f44575ad26b2cf48fee53e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer from: deepseek-ai/deepseek-r1-distill-qwen-32b\n"
     ]
    }
   ],
   "source": [
    "tok = AutoTokenizer.from_pretrained(\n",
    "    BASE_ID,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=TRUST_REMOTE_CODE\n",
    ")\n",
    "print(\"Loaded tokenizer from:\", BASE_ID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2c15de-0924-4810-ac95-a60956f3182a",
   "metadata": {},
   "source": [
    "# 4) Load base model (choose VRAM strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54cd8ce9-e194-4cfc-b791-5764d76b6a8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T21:09:31.698862Z",
     "iopub.status.busy": "2025-08-28T21:09:31.698587Z",
     "iopub.status.idle": "2025-08-28T21:14:24.747266Z",
     "shell.execute_reply": "2025-08-28T21:14:24.746699Z",
     "shell.execute_reply.started": "2025-08-28T21:09:31.698843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base with dtype=bfloat16.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5915e1ba612c47ce9008d46b1b834f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/664 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f61c7dae294e7d84177b5afc2f56df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2f097fcfb5465db8e9d2a91b34d5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-000008.safetensors:   0%|          | 0.00/8.79G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c4bbe546914e639ed1f837dffad7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-000008.safetensors:   0%|          | 0.00/8.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d337d5f382b4651a981ff7747743594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-000008.safetensors:   0%|          | 0.00/8.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a433a05764964e41b3a4769b2c72a337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-000008.safetensors:   0%|          | 0.00/8.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[2m2025-08-28T21:10:44.356186Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mStatus Code: 500. Retrying..., \u001b[1;33mrequest_id\u001b[0m\u001b[33m: \"\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs:220\n",
      "\n",
      "  \u001b[2m2025-08-28T21:10:44.357170Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mRetry attempt #0. Sleeping 1.489672063s before the next attempt\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /root/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs:171\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ed3d83752648a48160e3632cf32382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-000008.safetensors:   0%|          | 0.00/8.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6772ace7441489f990904d6c45f2778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-000008.safetensors:   0%|          | 0.00/8.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf857e925204f279278741407ecd286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-000008.safetensors:   0%|          | 0.00/8.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7355607372df43008df61baee9eef0a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-000008.safetensors:   0%|          | 0.00/4.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71dda609f1a543fe9be842ce3f6c7add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87045c2ea2d149a796cc63c8de06c006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded base model: deepseek-ai/deepseek-r1-distill-qwen-32b\n"
     ]
    }
   ],
   "source": [
    "load_kwargs = dict(\n",
    "    trust_remote_code=TRUST_REMOTE_CODE,\n",
    "    device_map=DEVICE_MAP,          # \"auto\" -> try GPU; \"cpu\" -> CPU-only\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "if USE_8BIT_BASE:\n",
    "    load_kwargs.update(dict(load_in_8bit=True))     # ~greatly reduces VRAM, slower\n",
    "    print(\"Loading base in 8-bit mode.\")\n",
    "else:\n",
    "    # Respect DTYPE if not using 8-bit\n",
    "    torch_dtype = {\n",
    "        \"auto\": \"auto\",\n",
    "        \"bfloat16\": torch.bfloat16,\n",
    "        \"float16\": torch.float16,\n",
    "        \"float32\": torch.float32,\n",
    "    }[DTYPE]\n",
    "    load_kwargs.update(dict(torch_dtype=(None if torch_dtype == \"auto\" else torch_dtype)))\n",
    "    print(f\"Loading base with dtype={DTYPE}.\")\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(BASE_ID, **load_kwargs)\n",
    "print(\"Loaded base model:\", BASE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ab6909-293c-44a3-9c23-9aaaea55e6a5",
   "metadata": {},
   "source": [
    "# 5) Load LoRA adapter on top of base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fb973b0-f26b-4277-a03e-85caa04eb99c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T21:17:56.445716Z",
     "iopub.status.busy": "2025-08-28T21:17:56.445438Z",
     "iopub.status.idle": "2025-08-28T21:17:59.499926Z",
     "shell.execute_reply": "2025-08-28T21:17:59.499333Z",
     "shell.execute_reply.started": "2025-08-28T21:17:56.445697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81eb2d72c575472da944a176c3cee6c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/879 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a007ab9e7f2412f8d239fdbe819611e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/33.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LoRA adapter: peers-ai/deepseek-32b-my-lora1-with-stops\n"
     ]
    }
   ],
   "source": [
    "# Your LoRA repo must contain adapter_model.safetensors + adapter_config.json\n",
    "peft_model = PeftModel.from_pretrained(base, ADAPTER_ID)\n",
    "print(\"Loaded LoRA adapter:\", ADAPTER_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30358b74-c55b-4aac-85c4-8ee445e6b202",
   "metadata": {},
   "source": [
    "# 6) Merge LoRA → base weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9208fe54-ea87-4e74-9f2a-d6c0666133e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T21:21:08.612023Z",
     "iopub.status.busy": "2025-08-28T21:21:08.611741Z",
     "iopub.status.idle": "2025-08-28T21:21:53.703783Z",
     "shell.execute_reply": "2025-08-28T21:21:53.703230Z",
     "shell.execute_reply.started": "2025-08-28T21:21:08.612004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging LoRA into base (can take a while)…\n",
      "Merge complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Merging LoRA into base (can take a while)…\")\n",
    "merged = peft_model.merge_and_unload()\n",
    "\n",
    "# Free GPU VRAM before saving (optional but helpful)\n",
    "merged = merged.to(\"cpu\")\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "print(\"Merge complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0398fde1-33dc-4711-ab7f-eebe56131bc2",
   "metadata": {},
   "source": [
    "# 7) Save the merged model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cfa2f3-9f97-4a8f-a57b-db187194a720",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "merged.save_pretrained(OUT_DIR, safe_serialization=True)\n",
    "tok.save_pretrained(OUT_DIR)\n",
    "\n",
    "print(\"Saved merged model to:\", OUT_DIR)\n",
    "# After this, OUT_DIR is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04eaf12-c142-4d61-8693-daf81b53759a",
   "metadata": {},
   "source": [
    "8) (Optional) Push the merged model to your Hugging Face repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c1b39e-a3a6-450e-8d43-d5de3431d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PUSH_TO:\n",
    "    if not HF_TOKEN:\n",
    "        raise ValueError(\"Set HF_TOKEN (env or cell 2) to push to the Hub.\")\n",
    "\n",
    "    api = HfApi()\n",
    "    try:\n",
    "        api.create_repo(PUSH_TO, private=False, exist_ok=True)\n",
    "        print(f\"Ensured HF repo exists: {PUSH_TO}\")\n",
    "    except Exception as e:\n",
    "        print(\"(Info) create_repo:\", e)\n",
    "\n",
    "    api.upload_folder(\n",
    "        folder_path=OUT_DIR,\n",
    "        repo_id=PUSH_TO,\n",
    "        commit_message=\"Add merged LoRA model\"\n",
    "    )\n",
    "    print(f\"Pushed merged model to: https://huggingface.co/{PUSH_TO}\")\n",
    "else:\n",
    "    print(\"Skipping push to Hub (PUSH_TO not set).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb95ac6-2246-4a1a-bc11-287be75835e6",
   "metadata": {},
   "source": [
    "9) Quick sanity test (local load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc6eb0d-645a-4973-9be5-cb23d0ecf331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload from OUT_DIR to verify it’s a standalone model\n",
    "test_tok = AutoTokenizer.from_pretrained(OUT_DIR, use_fast=True, trust_remote_code=TRUST_REMOTE_CODE)\n",
    "test_model = AutoModelForCausalLM.from_pretrained(OUT_DIR, torch_dtype=torch.bfloat16 if DTYPE==\"bfloat16\" else \"auto\", device_map=\"auto\", trust_remote_code=TRUST_REMOTE_CODE)\n",
    "\n",
    "prompt = \"You are a helpful assistant. Briefly introduce yourself.\"\n",
    "inputs = test_tok(prompt, return_tensors=\"pt\").to(next(test_model.parameters()).device)\n",
    "with torch.no_grad():\n",
    "    out = test_model.generate(**inputs, max_new_tokens=64)\n",
    "print(test_tok.decode(out[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
